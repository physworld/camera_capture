{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "specialized-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization,AveragePooling2D\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "union-natural",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   emotion                                             pixels     Usage\n",
      "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
      "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
      "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
      "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
      "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training\n"
     ]
    }
   ],
   "source": [
    "#load data and look at the df.\n",
    "\n",
    "df=pd.read_csv('fer2013.csv')\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "apart-microwave",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build the training and test sets \n",
    "X_train,train_y,X_test,test_y=[],[],[],[]\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    val=row['pixels'].split(\" \")\n",
    "    try:\n",
    "        if 'Training' in row['Usage']:\n",
    "           X_train.append(np.array(val,'float32'))\n",
    "           train_y.append(row['emotion'])\n",
    "        elif 'PublicTest' in row['Usage']:\n",
    "           X_test.append(np.array(val,'float32'))\n",
    "           test_y.append(row['emotion'])\n",
    "    except:\n",
    "        print(f\"error occured at index :{index} and row:{row}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "married-welding",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set hyperparameters and preprocess data.\n",
    "\n",
    "num_features = 64\n",
    "num_labels = 7\n",
    "batch_size = 64\n",
    "epochs = 200\n",
    "width, height = 48, 48\n",
    "\n",
    "X_train = np.array(X_train,'float32')\n",
    "train_y = np.array(train_y,'float32')\n",
    "X_test = np.array(X_test,'float32')\n",
    "test_y = np.array(test_y,'float32')\n",
    "\n",
    "train_y=utils.to_categorical(train_y, num_classes=num_labels)\n",
    "test_y=utils.to_categorical(test_y, num_classes=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "hired-trigger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:(28709, 48, 48, 1)\n"
     ]
    }
   ],
   "source": [
    "#normalizing data between 0 and 1\n",
    "X_train -= np.mean(X_train, axis=0)\n",
    "X_train /= np.std(X_train, axis=0)\n",
    "\n",
    "X_test -= np.mean(X_test, axis=0)\n",
    "X_test /= np.std(X_test, axis=0)\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 48, 48, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 48, 48, 1)\n",
    "\n",
    "print(f\"shape:{X_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "enhanced-interest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 46, 46, 64)        640       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 44, 44, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 20, 20, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 18, 18, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 9, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 9, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 7, 7, 256)         295168    \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 5, 5, 256)         590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 7)                 7175      \n",
      "=================================================================\n",
      "Total params: 3,250,631\n",
      "Trainable params: 3,250,631\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "##designing the cnn\n",
    "#1st convolution layer\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=(X_train.shape[1:])))\n",
    "model.add(Conv2D(64,kernel_size= (3, 3), activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#2nd convolution layer\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#3rd convolution layer\n",
    "model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "#fully connected neural networks\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(num_labels, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "second-syndicate",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compliling the model\n",
    "model.compile(loss=categorical_crossentropy,\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "arranged-control",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "449/449 [==============================] - 199s 441ms/step - loss: 1.8051 - accuracy: 0.2536 - val_loss: 1.5691 - val_accuracy: 0.3678\n",
      "Epoch 2/200\n",
      "449/449 [==============================] - 195s 434ms/step - loss: 1.5574 - accuracy: 0.3815 - val_loss: 1.4222 - val_accuracy: 0.4494\n",
      "Epoch 3/200\n",
      "449/449 [==============================] - 194s 432ms/step - loss: 1.4196 - accuracy: 0.4494 - val_loss: 1.3117 - val_accuracy: 0.4987\n",
      "Epoch 4/200\n",
      "449/449 [==============================] - 196s 436ms/step - loss: 1.3341 - accuracy: 0.4814 - val_loss: 1.2975 - val_accuracy: 0.4993\n",
      "Epoch 5/200\n",
      "449/449 [==============================] - 194s 432ms/step - loss: 1.2807 - accuracy: 0.5019 - val_loss: 1.2502 - val_accuracy: 0.5132\n",
      "Epoch 6/200\n",
      "449/449 [==============================] - 205s 456ms/step - loss: 1.2410 - accuracy: 0.5243 - val_loss: 1.2127 - val_accuracy: 0.5481\n",
      "Epoch 7/200\n",
      "449/449 [==============================] - 204s 454ms/step - loss: 1.1953 - accuracy: 0.5470 - val_loss: 1.1944 - val_accuracy: 0.5481\n",
      "Epoch 8/200\n",
      "449/449 [==============================] - 222s 493ms/step - loss: 1.1683 - accuracy: 0.5553 - val_loss: 1.1983 - val_accuracy: 0.5369\n",
      "Epoch 9/200\n",
      "449/449 [==============================] - 257s 572ms/step - loss: 1.1450 - accuracy: 0.5649 - val_loss: 1.1927 - val_accuracy: 0.5520\n",
      "Epoch 10/200\n",
      "449/449 [==============================] - 235s 523ms/step - loss: 1.1129 - accuracy: 0.5738 - val_loss: 1.1576 - val_accuracy: 0.5659\n",
      "Epoch 11/200\n",
      "449/449 [==============================] - 227s 506ms/step - loss: 1.0800 - accuracy: 0.5867 - val_loss: 1.1656 - val_accuracy: 0.5581\n",
      "Epoch 12/200\n",
      "449/449 [==============================] - 228s 508ms/step - loss: 1.0737 - accuracy: 0.5859 - val_loss: 1.1221 - val_accuracy: 0.5667\n",
      "Epoch 13/200\n",
      "449/449 [==============================] - 223s 497ms/step - loss: 1.0325 - accuracy: 0.6086 - val_loss: 1.1440 - val_accuracy: 0.5667\n",
      "Epoch 14/200\n",
      "449/449 [==============================] - 223s 498ms/step - loss: 1.0176 - accuracy: 0.6130 - val_loss: 1.1660 - val_accuracy: 0.5587\n",
      "Epoch 15/200\n",
      "449/449 [==============================] - 216s 481ms/step - loss: 0.9892 - accuracy: 0.6215 - val_loss: 1.1674 - val_accuracy: 0.5626\n",
      "Epoch 16/200\n",
      "449/449 [==============================] - 198s 441ms/step - loss: 0.9654 - accuracy: 0.6358 - val_loss: 1.1519 - val_accuracy: 0.5729\n",
      "Epoch 17/200\n",
      "449/449 [==============================] - 204s 454ms/step - loss: 0.9466 - accuracy: 0.6358 - val_loss: 1.1492 - val_accuracy: 0.5762\n",
      "Epoch 18/200\n",
      "449/449 [==============================] - 196s 438ms/step - loss: 0.9389 - accuracy: 0.6440 - val_loss: 1.1543 - val_accuracy: 0.5773\n",
      "Epoch 19/200\n",
      "449/449 [==============================] - 193s 430ms/step - loss: 0.9060 - accuracy: 0.6538 - val_loss: 1.1899 - val_accuracy: 0.5801\n",
      "Epoch 20/200\n",
      "449/449 [==============================] - 212s 472ms/step - loss: 0.8780 - accuracy: 0.6645 - val_loss: 1.1582 - val_accuracy: 0.5868\n",
      "Epoch 21/200\n",
      "449/449 [==============================] - 248s 552ms/step - loss: 0.8712 - accuracy: 0.6709 - val_loss: 1.1651 - val_accuracy: 0.5832\n",
      "Epoch 22/200\n",
      "449/449 [==============================] - 247s 550ms/step - loss: 0.8476 - accuracy: 0.6803 - val_loss: 1.1880 - val_accuracy: 0.5734\n",
      "Epoch 23/200\n",
      "449/449 [==============================] - 239s 533ms/step - loss: 0.8204 - accuracy: 0.6851 - val_loss: 1.2109 - val_accuracy: 0.5609\n",
      "Epoch 24/200\n",
      "449/449 [==============================] - 222s 495ms/step - loss: 0.8261 - accuracy: 0.6828 - val_loss: 1.1924 - val_accuracy: 0.5818\n",
      "Epoch 25/200\n",
      "449/449 [==============================] - 232s 518ms/step - loss: 0.7972 - accuracy: 0.7017 - val_loss: 1.2200 - val_accuracy: 0.5823\n",
      "Epoch 26/200\n",
      "449/449 [==============================] - 205s 457ms/step - loss: 0.7976 - accuracy: 0.7022 - val_loss: 1.2276 - val_accuracy: 0.5807\n",
      "Epoch 27/200\n",
      "449/449 [==============================] - 203s 452ms/step - loss: 0.7609 - accuracy: 0.7082 - val_loss: 1.2758 - val_accuracy: 0.5676\n",
      "Epoch 28/200\n",
      "449/449 [==============================] - 209s 466ms/step - loss: 0.7449 - accuracy: 0.7222 - val_loss: 1.2162 - val_accuracy: 0.5846\n",
      "Epoch 29/200\n",
      "449/449 [==============================] - 243s 541ms/step - loss: 0.7392 - accuracy: 0.7202 - val_loss: 1.2489 - val_accuracy: 0.5756\n",
      "Epoch 30/200\n",
      "449/449 [==============================] - 235s 523ms/step - loss: 0.7271 - accuracy: 0.7244 - val_loss: 1.2904 - val_accuracy: 0.5779\n",
      "Epoch 31/200\n",
      "449/449 [==============================] - 217s 483ms/step - loss: 0.7019 - accuracy: 0.7363 - val_loss: 1.2540 - val_accuracy: 0.5851\n",
      "Epoch 32/200\n",
      "449/449 [==============================] - 243s 542ms/step - loss: 0.6944 - accuracy: 0.7410 - val_loss: 1.2721 - val_accuracy: 0.5784\n",
      "Epoch 33/200\n",
      "449/449 [==============================] - 225s 502ms/step - loss: 0.6695 - accuracy: 0.7498 - val_loss: 1.2823 - val_accuracy: 0.5882\n",
      "Epoch 34/200\n",
      "449/449 [==============================] - 235s 524ms/step - loss: 0.6692 - accuracy: 0.7495 - val_loss: 1.3008 - val_accuracy: 0.5751\n",
      "Epoch 35/200\n",
      "449/449 [==============================] - 239s 531ms/step - loss: 0.6562 - accuracy: 0.7567 - val_loss: 1.3435 - val_accuracy: 0.5843\n",
      "Epoch 36/200\n",
      "449/449 [==============================] - 218s 487ms/step - loss: 0.6582 - accuracy: 0.7507 - val_loss: 1.3015 - val_accuracy: 0.5834\n",
      "Epoch 37/200\n",
      "449/449 [==============================] - 243s 541ms/step - loss: 0.6416 - accuracy: 0.7619 - val_loss: 1.3076 - val_accuracy: 0.5687\n",
      "Epoch 38/200\n",
      "449/449 [==============================] - 232s 517ms/step - loss: 0.6155 - accuracy: 0.7711 - val_loss: 1.3586 - val_accuracy: 0.5751\n",
      "Epoch 39/200\n",
      "449/449 [==============================] - 220s 489ms/step - loss: 0.6168 - accuracy: 0.7695 - val_loss: 1.3272 - val_accuracy: 0.5748\n",
      "Epoch 40/200\n",
      "449/449 [==============================] - 237s 528ms/step - loss: 0.5956 - accuracy: 0.7761 - val_loss: 1.3219 - val_accuracy: 0.5737\n",
      "Epoch 41/200\n",
      "449/449 [==============================] - 238s 531ms/step - loss: 0.5996 - accuracy: 0.7799 - val_loss: 1.4349 - val_accuracy: 0.5790\n",
      "Epoch 42/200\n",
      "449/449 [==============================] - 268s 596ms/step - loss: 0.5980 - accuracy: 0.7860 - val_loss: 1.4062 - val_accuracy: 0.5779\n",
      "Epoch 43/200\n",
      "449/449 [==============================] - 241s 537ms/step - loss: 0.5822 - accuracy: 0.7859 - val_loss: 1.3736 - val_accuracy: 0.5846\n",
      "Epoch 44/200\n",
      "449/449 [==============================] - 262s 584ms/step - loss: 0.5537 - accuracy: 0.7942 - val_loss: 1.3643 - val_accuracy: 0.5876\n",
      "Epoch 45/200\n",
      "449/449 [==============================] - 232s 518ms/step - loss: 0.5466 - accuracy: 0.7988 - val_loss: 1.4574 - val_accuracy: 0.5712\n",
      "Epoch 46/200\n",
      "449/449 [==============================] - 218s 485ms/step - loss: 0.5489 - accuracy: 0.7944 - val_loss: 1.4414 - val_accuracy: 0.5740\n",
      "Epoch 47/200\n",
      "449/449 [==============================] - 216s 481ms/step - loss: 0.5438 - accuracy: 0.7986 - val_loss: 1.5086 - val_accuracy: 0.5790\n",
      "Epoch 48/200\n",
      "449/449 [==============================] - 200s 444ms/step - loss: 0.5427 - accuracy: 0.7997 - val_loss: 1.4429 - val_accuracy: 0.5795\n",
      "Epoch 49/200\n",
      "449/449 [==============================] - 190s 423ms/step - loss: 0.5184 - accuracy: 0.8121 - val_loss: 1.5038 - val_accuracy: 0.5712\n",
      "Epoch 50/200\n",
      "449/449 [==============================] - 191s 425ms/step - loss: 0.5081 - accuracy: 0.8159 - val_loss: 1.3945 - val_accuracy: 0.5815\n",
      "Epoch 51/200\n",
      "449/449 [==============================] - 197s 438ms/step - loss: 0.5079 - accuracy: 0.8160 - val_loss: 1.5101 - val_accuracy: 0.5745\n",
      "Epoch 52/200\n",
      "449/449 [==============================] - 195s 435ms/step - loss: 0.5040 - accuracy: 0.8213 - val_loss: 1.5402 - val_accuracy: 0.5846\n",
      "Epoch 53/200\n",
      "449/449 [==============================] - 192s 428ms/step - loss: 0.5039 - accuracy: 0.8188 - val_loss: 1.5216 - val_accuracy: 0.5801\n",
      "Epoch 54/200\n",
      "449/449 [==============================] - 191s 424ms/step - loss: 0.4835 - accuracy: 0.8261 - val_loss: 1.5339 - val_accuracy: 0.5701\n",
      "Epoch 55/200\n",
      "449/449 [==============================] - 186s 415ms/step - loss: 0.5024 - accuracy: 0.8184 - val_loss: 1.5457 - val_accuracy: 0.5768\n",
      "Epoch 56/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "449/449 [==============================] - 190s 424ms/step - loss: 0.4781 - accuracy: 0.8289 - val_loss: 1.6112 - val_accuracy: 0.5762\n",
      "Epoch 57/200\n",
      "449/449 [==============================] - 192s 428ms/step - loss: 0.4667 - accuracy: 0.8314 - val_loss: 1.5509 - val_accuracy: 0.5756\n",
      "Epoch 58/200\n",
      "449/449 [==============================] - 191s 425ms/step - loss: 0.4758 - accuracy: 0.8289 - val_loss: 1.6487 - val_accuracy: 0.5762\n",
      "Epoch 59/200\n",
      "449/449 [==============================] - 192s 428ms/step - loss: 0.4466 - accuracy: 0.8386 - val_loss: 1.5875 - val_accuracy: 0.5648\n",
      "Epoch 60/200\n",
      "449/449 [==============================] - 193s 430ms/step - loss: 0.4503 - accuracy: 0.8362 - val_loss: 1.6135 - val_accuracy: 0.5840\n",
      "Epoch 61/200\n",
      "449/449 [==============================] - 183s 408ms/step - loss: 0.4437 - accuracy: 0.8424 - val_loss: 1.6156 - val_accuracy: 0.5798\n",
      "Epoch 62/200\n",
      "449/449 [==============================] - 186s 415ms/step - loss: 0.4301 - accuracy: 0.8455 - val_loss: 1.5974 - val_accuracy: 0.5773\n",
      "Epoch 63/200\n",
      "449/449 [==============================] - 183s 408ms/step - loss: 0.4397 - accuracy: 0.8440 - val_loss: 1.5807 - val_accuracy: 0.5784\n",
      "Epoch 64/200\n",
      "449/449 [==============================] - 193s 431ms/step - loss: 0.4287 - accuracy: 0.8446 - val_loss: 1.6653 - val_accuracy: 0.5773\n",
      "Epoch 65/200\n",
      "449/449 [==============================] - 184s 410ms/step - loss: 0.4275 - accuracy: 0.8501 - val_loss: 1.6842 - val_accuracy: 0.5690\n",
      "Epoch 66/200\n",
      "449/449 [==============================] - 23424s 52s/step - loss: 0.4284 - accuracy: 0.8505 - val_loss: 1.6204 - val_accuracy: 0.5709\n",
      "Epoch 67/200\n",
      "449/449 [==============================] - 182s 406ms/step - loss: 0.4201 - accuracy: 0.8538 - val_loss: 1.5997 - val_accuracy: 0.5784\n",
      "Epoch 68/200\n",
      "449/449 [==============================] - 180s 402ms/step - loss: 0.4213 - accuracy: 0.8495 - val_loss: 1.5928 - val_accuracy: 0.5751\n",
      "Epoch 69/200\n",
      "449/449 [==============================] - 180s 400ms/step - loss: 0.4095 - accuracy: 0.8524 - val_loss: 1.5926 - val_accuracy: 0.5790\n",
      "Epoch 70/200\n",
      "449/449 [==============================] - 180s 400ms/step - loss: 0.4076 - accuracy: 0.8575 - val_loss: 1.6790 - val_accuracy: 0.5701\n",
      "Epoch 71/200\n",
      "449/449 [==============================] - 180s 400ms/step - loss: 0.4046 - accuracy: 0.8580 - val_loss: 1.6453 - val_accuracy: 0.5784\n",
      "Epoch 72/200\n",
      "449/449 [==============================] - 179s 399ms/step - loss: 0.3965 - accuracy: 0.8636 - val_loss: 1.6764 - val_accuracy: 0.5723\n",
      "Epoch 73/200\n",
      "449/449 [==============================] - 179s 400ms/step - loss: 0.3879 - accuracy: 0.8657 - val_loss: 1.6554 - val_accuracy: 0.5784\n",
      "Epoch 74/200\n",
      "449/449 [==============================] - 180s 400ms/step - loss: 0.3881 - accuracy: 0.8635 - val_loss: 1.7803 - val_accuracy: 0.5745\n",
      "Epoch 75/200\n",
      "449/449 [==============================] - 179s 400ms/step - loss: 0.3918 - accuracy: 0.8622 - val_loss: 1.6788 - val_accuracy: 0.5759\n",
      "Epoch 76/200\n",
      "449/449 [==============================] - 180s 400ms/step - loss: 0.3822 - accuracy: 0.8663 - val_loss: 1.7444 - val_accuracy: 0.5782\n",
      "Epoch 77/200\n",
      "449/449 [==============================] - 179s 399ms/step - loss: 0.3820 - accuracy: 0.8658 - val_loss: 1.7484 - val_accuracy: 0.5821\n",
      "Epoch 78/200\n",
      "449/449 [==============================] - 179s 400ms/step - loss: 0.3950 - accuracy: 0.8648 - val_loss: 1.7414 - val_accuracy: 0.5751\n",
      "Epoch 79/200\n",
      "449/449 [==============================] - 180s 400ms/step - loss: 0.3787 - accuracy: 0.8702 - val_loss: 1.7186 - val_accuracy: 0.5804\n",
      "Epoch 80/200\n",
      "449/449 [==============================] - 179s 399ms/step - loss: 0.3722 - accuracy: 0.8721 - val_loss: 1.7492 - val_accuracy: 0.5687\n",
      "Epoch 81/200\n",
      "449/449 [==============================] - 179s 400ms/step - loss: 0.3642 - accuracy: 0.8701 - val_loss: 1.7805 - val_accuracy: 0.5762\n",
      "Epoch 82/200\n",
      "449/449 [==============================] - 179s 400ms/step - loss: 0.3601 - accuracy: 0.8771 - val_loss: 1.8575 - val_accuracy: 0.5815\n",
      "Epoch 83/200\n",
      "449/449 [==============================] - 181s 403ms/step - loss: 0.3509 - accuracy: 0.8778 - val_loss: 1.8249 - val_accuracy: 0.5840\n",
      "Epoch 84/200\n",
      "449/449 [==============================] - 181s 402ms/step - loss: 0.3707 - accuracy: 0.8724 - val_loss: 1.6312 - val_accuracy: 0.5821\n",
      "Epoch 85/200\n",
      "449/449 [==============================] - 190s 424ms/step - loss: 0.3717 - accuracy: 0.8716 - val_loss: 1.7710 - val_accuracy: 0.5776\n",
      "Epoch 86/200\n",
      "449/449 [==============================] - 216s 481ms/step - loss: 0.3539 - accuracy: 0.8767 - val_loss: 1.7316 - val_accuracy: 0.5754\n",
      "Epoch 87/200\n",
      "449/449 [==============================] - 213s 474ms/step - loss: 0.3477 - accuracy: 0.8798 - val_loss: 1.7581 - val_accuracy: 0.5723\n",
      "Epoch 88/200\n",
      "449/449 [==============================] - 206s 460ms/step - loss: 0.3465 - accuracy: 0.8805 - val_loss: 1.8285 - val_accuracy: 0.5834\n",
      "Epoch 89/200\n",
      "449/449 [==============================] - 188s 420ms/step - loss: 0.3294 - accuracy: 0.8863 - val_loss: 1.8130 - val_accuracy: 0.5678\n",
      "Epoch 90/200\n",
      "449/449 [==============================] - 188s 419ms/step - loss: 0.3376 - accuracy: 0.8845 - val_loss: 1.8733 - val_accuracy: 0.5701\n",
      "Epoch 91/200\n",
      "449/449 [==============================] - 208s 464ms/step - loss: 0.3573 - accuracy: 0.8813 - val_loss: 1.8001 - val_accuracy: 0.5834\n",
      "Epoch 92/200\n",
      "449/449 [==============================] - 236s 525ms/step - loss: 0.3333 - accuracy: 0.8808 - val_loss: 1.9044 - val_accuracy: 0.5770\n",
      "Epoch 93/200\n",
      "449/449 [==============================] - 240s 534ms/step - loss: 0.3406 - accuracy: 0.8842 - val_loss: 1.8859 - val_accuracy: 0.5748\n",
      "Epoch 94/200\n",
      "449/449 [==============================] - 232s 517ms/step - loss: 0.3417 - accuracy: 0.8831 - val_loss: 1.7743 - val_accuracy: 0.5709\n",
      "Epoch 95/200\n",
      "449/449 [==============================] - 248s 553ms/step - loss: 0.3298 - accuracy: 0.8831 - val_loss: 1.8274 - val_accuracy: 0.5756\n",
      "Epoch 96/200\n",
      "449/449 [==============================] - 242s 540ms/step - loss: 0.3252 - accuracy: 0.8882 - val_loss: 1.8546 - val_accuracy: 0.5756\n",
      "Epoch 97/200\n",
      "449/449 [==============================] - 235s 523ms/step - loss: 0.3230 - accuracy: 0.8868 - val_loss: 1.8809 - val_accuracy: 0.5818\n",
      "Epoch 98/200\n",
      "449/449 [==============================] - 241s 536ms/step - loss: 0.3471 - accuracy: 0.8840 - val_loss: 1.8858 - val_accuracy: 0.5743\n",
      "Epoch 99/200\n",
      "449/449 [==============================] - 231s 514ms/step - loss: 0.3185 - accuracy: 0.8932 - val_loss: 1.8650 - val_accuracy: 0.5759\n",
      "Epoch 100/200\n",
      "449/449 [==============================] - 248s 552ms/step - loss: 0.3083 - accuracy: 0.8953 - val_loss: 1.9202 - val_accuracy: 0.5723\n",
      "Epoch 101/200\n",
      "449/449 [==============================] - 246s 549ms/step - loss: 0.3272 - accuracy: 0.8866 - val_loss: 1.8464 - val_accuracy: 0.5759\n",
      "Epoch 102/200\n",
      "449/449 [==============================] - 229s 510ms/step - loss: 0.3213 - accuracy: 0.8900 - val_loss: 1.8154 - val_accuracy: 0.5779\n",
      "Epoch 103/200\n",
      "449/449 [==============================] - 251s 559ms/step - loss: 0.3176 - accuracy: 0.8921 - val_loss: 1.9529 - val_accuracy: 0.5734\n",
      "Epoch 104/200\n",
      "449/449 [==============================] - 243s 540ms/step - loss: 0.3296 - accuracy: 0.8888 - val_loss: 1.8395 - val_accuracy: 0.5773\n",
      "Epoch 105/200\n",
      "449/449 [==============================] - 248s 553ms/step - loss: 0.3174 - accuracy: 0.8943 - val_loss: 1.8033 - val_accuracy: 0.5826\n",
      "Epoch 106/200\n",
      "449/449 [==============================] - 245s 545ms/step - loss: 0.3123 - accuracy: 0.8953 - val_loss: 1.9015 - val_accuracy: 0.5809\n",
      "Epoch 107/200\n",
      "449/449 [==============================] - 220s 490ms/step - loss: 0.3218 - accuracy: 0.8905 - val_loss: 1.8998 - val_accuracy: 0.5826\n",
      "Epoch 108/200\n",
      "449/449 [==============================] - 249s 555ms/step - loss: 0.3269 - accuracy: 0.8907 - val_loss: 1.8548 - val_accuracy: 0.5731\n",
      "Epoch 109/200\n",
      "449/449 [==============================] - 225s 501ms/step - loss: 0.2813 - accuracy: 0.9041 - val_loss: 1.9240 - val_accuracy: 0.5731\n",
      "Epoch 110/200\n",
      "449/449 [==============================] - 250s 558ms/step - loss: 0.3094 - accuracy: 0.8943 - val_loss: 1.8414 - val_accuracy: 0.5709\n",
      "Epoch 111/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "449/449 [==============================] - 250s 557ms/step - loss: 0.3192 - accuracy: 0.8925 - val_loss: 1.8958 - val_accuracy: 0.5717\n",
      "Epoch 112/200\n",
      "449/449 [==============================] - 226s 503ms/step - loss: 0.2939 - accuracy: 0.9013 - val_loss: 1.9032 - val_accuracy: 0.5712\n",
      "Epoch 113/200\n",
      "449/449 [==============================] - 182s 405ms/step - loss: 0.3019 - accuracy: 0.8967 - val_loss: 1.9157 - val_accuracy: 0.5673\n",
      "Epoch 114/200\n",
      "449/449 [==============================] - 182s 405ms/step - loss: 0.3077 - accuracy: 0.8991 - val_loss: 1.8880 - val_accuracy: 0.5723\n",
      "Epoch 115/200\n",
      "449/449 [==============================] - 181s 402ms/step - loss: 0.2961 - accuracy: 0.8991 - val_loss: 1.8203 - val_accuracy: 0.5779\n",
      "Epoch 116/200\n",
      "449/449 [==============================] - 180s 402ms/step - loss: 0.3090 - accuracy: 0.8961 - val_loss: 1.8391 - val_accuracy: 0.5860\n",
      "Epoch 117/200\n",
      "449/449 [==============================] - 181s 402ms/step - loss: 0.3010 - accuracy: 0.8974 - val_loss: 1.9709 - val_accuracy: 0.5793\n",
      "Epoch 118/200\n",
      "449/449 [==============================] - 182s 405ms/step - loss: 0.2874 - accuracy: 0.9057 - val_loss: 1.9420 - val_accuracy: 0.5798\n",
      "Epoch 119/200\n",
      "449/449 [==============================] - 180s 400ms/step - loss: 0.2891 - accuracy: 0.9016 - val_loss: 1.9535 - val_accuracy: 0.5756\n",
      "Epoch 120/200\n",
      "449/449 [==============================] - 180s 401ms/step - loss: 0.2816 - accuracy: 0.9075 - val_loss: 1.9621 - val_accuracy: 0.5717\n",
      "Epoch 121/200\n",
      "449/449 [==============================] - 180s 401ms/step - loss: 0.2753 - accuracy: 0.9033 - val_loss: 1.9360 - val_accuracy: 0.5779\n",
      "Epoch 122/200\n",
      "449/449 [==============================] - 180s 401ms/step - loss: 0.2892 - accuracy: 0.9048 - val_loss: 1.8685 - val_accuracy: 0.5754\n",
      "Epoch 123/200\n",
      "449/449 [==============================] - 179s 398ms/step - loss: 0.3101 - accuracy: 0.8961 - val_loss: 1.9475 - val_accuracy: 0.5695\n",
      "Epoch 124/200\n",
      "449/449 [==============================] - 179s 398ms/step - loss: 0.2978 - accuracy: 0.9033 - val_loss: 1.8941 - val_accuracy: 0.5743\n",
      "Epoch 125/200\n",
      "449/449 [==============================] - 180s 401ms/step - loss: 0.2885 - accuracy: 0.9055 - val_loss: 1.9313 - val_accuracy: 0.5804\n",
      "Epoch 126/200\n",
      "449/449 [==============================] - 179s 399ms/step - loss: 0.2944 - accuracy: 0.9020 - val_loss: 1.9075 - val_accuracy: 0.5795\n",
      "Epoch 127/200\n",
      "449/449 [==============================] - 179s 399ms/step - loss: 0.2758 - accuracy: 0.9064 - val_loss: 1.8211 - val_accuracy: 0.5829\n",
      "Epoch 128/200\n",
      "449/449 [==============================] - 180s 400ms/step - loss: 0.2838 - accuracy: 0.9051 - val_loss: 1.9538 - val_accuracy: 0.5837\n",
      "Epoch 129/200\n",
      "449/449 [==============================] - 179s 398ms/step - loss: 0.2939 - accuracy: 0.9003 - val_loss: 1.9057 - val_accuracy: 0.5768\n",
      "Epoch 130/200\n",
      "449/449 [==============================] - 180s 401ms/step - loss: 0.2923 - accuracy: 0.9054 - val_loss: 1.8666 - val_accuracy: 0.5798\n",
      "Epoch 131/200\n",
      "449/449 [==============================] - 179s 399ms/step - loss: 0.3236 - accuracy: 0.8915 - val_loss: 2.0293 - val_accuracy: 0.5690\n",
      "Epoch 132/200\n",
      "449/449 [==============================] - 506s 1s/step - loss: 0.2862 - accuracy: 0.9054 - val_loss: 1.9593 - val_accuracy: 0.5812\n",
      "Epoch 133/200\n",
      "449/449 [==============================] - 191s 426ms/step - loss: 0.2859 - accuracy: 0.9064 - val_loss: 1.9298 - val_accuracy: 0.5756\n",
      "Epoch 134/200\n",
      "449/449 [==============================] - 185s 411ms/step - loss: 0.2827 - accuracy: 0.9075 - val_loss: 2.0542 - val_accuracy: 0.5748\n",
      "Epoch 135/200\n",
      "449/449 [==============================] - 186s 413ms/step - loss: 0.2640 - accuracy: 0.9087 - val_loss: 1.9963 - val_accuracy: 0.5762\n",
      "Epoch 136/200\n",
      "449/449 [==============================] - 186s 414ms/step - loss: 0.2800 - accuracy: 0.9066 - val_loss: 2.0640 - val_accuracy: 0.5726\n",
      "Epoch 137/200\n",
      "449/449 [==============================] - 185s 412ms/step - loss: 0.2762 - accuracy: 0.9107 - val_loss: 1.9437 - val_accuracy: 0.5651\n",
      "Epoch 138/200\n",
      "449/449 [==============================] - 183s 408ms/step - loss: 0.2730 - accuracy: 0.9120 - val_loss: 2.0576 - val_accuracy: 0.5698\n",
      "Epoch 139/200\n",
      "449/449 [==============================] - 182s 405ms/step - loss: 0.2902 - accuracy: 0.9042 - val_loss: 1.8928 - val_accuracy: 0.5751\n",
      "Epoch 140/200\n",
      "449/449 [==============================] - 182s 405ms/step - loss: 0.2779 - accuracy: 0.9053 - val_loss: 2.0540 - val_accuracy: 0.5606\n",
      "Epoch 141/200\n",
      "449/449 [==============================] - 182s 406ms/step - loss: 0.2688 - accuracy: 0.9104 - val_loss: 2.0470 - val_accuracy: 0.5701\n",
      "Epoch 142/200\n",
      "449/449 [==============================] - 182s 405ms/step - loss: 0.2831 - accuracy: 0.9055 - val_loss: 1.9528 - val_accuracy: 0.5709\n",
      "Epoch 143/200\n",
      "449/449 [==============================] - 182s 405ms/step - loss: 0.2687 - accuracy: 0.9120 - val_loss: 1.8869 - val_accuracy: 0.5765\n",
      "Epoch 144/200\n",
      "449/449 [==============================] - 182s 405ms/step - loss: 0.2827 - accuracy: 0.9083 - val_loss: 1.9821 - val_accuracy: 0.5723\n",
      "Epoch 145/200\n",
      "449/449 [==============================] - 182s 405ms/step - loss: 0.2459 - accuracy: 0.9185 - val_loss: 2.0590 - val_accuracy: 0.5603\n",
      "Epoch 146/200\n",
      "449/449 [==============================] - 181s 404ms/step - loss: 0.2777 - accuracy: 0.9090 - val_loss: 2.0025 - val_accuracy: 0.5734\n",
      "Epoch 147/200\n",
      "449/449 [==============================] - 182s 405ms/step - loss: 0.2770 - accuracy: 0.9087 - val_loss: 1.9381 - val_accuracy: 0.5826\n",
      "Epoch 148/200\n",
      "449/449 [==============================] - 182s 405ms/step - loss: 0.2595 - accuracy: 0.9147 - val_loss: 1.8859 - val_accuracy: 0.5773\n",
      "Epoch 149/200\n",
      "449/449 [==============================] - 182s 405ms/step - loss: 0.2690 - accuracy: 0.9125 - val_loss: 1.9302 - val_accuracy: 0.5784\n",
      "Epoch 150/200\n",
      "449/449 [==============================] - 182s 406ms/step - loss: 0.2656 - accuracy: 0.9147 - val_loss: 2.0869 - val_accuracy: 0.5720\n",
      "Epoch 151/200\n",
      "449/449 [==============================] - 182s 405ms/step - loss: 0.2749 - accuracy: 0.9108 - val_loss: 2.0663 - val_accuracy: 0.5801\n",
      "Epoch 152/200\n",
      "449/449 [==============================] - 182s 405ms/step - loss: 0.2706 - accuracy: 0.9127 - val_loss: 1.9524 - val_accuracy: 0.5748\n",
      "Epoch 153/200\n",
      "449/449 [==============================] - 182s 406ms/step - loss: 0.2689 - accuracy: 0.9132 - val_loss: 2.0217 - val_accuracy: 0.5798\n",
      "Epoch 154/200\n",
      "449/449 [==============================] - 184s 409ms/step - loss: 0.2535 - accuracy: 0.9180 - val_loss: 1.8572 - val_accuracy: 0.5790\n",
      "Epoch 155/200\n",
      "449/449 [==============================] - 182s 405ms/step - loss: 0.2682 - accuracy: 0.9122 - val_loss: 1.8713 - val_accuracy: 0.5776\n",
      "Epoch 156/200\n",
      "449/449 [==============================] - 183s 408ms/step - loss: 0.2607 - accuracy: 0.9167 - val_loss: 2.0235 - val_accuracy: 0.5695\n",
      "Epoch 157/200\n",
      "449/449 [==============================] - 182s 406ms/step - loss: 0.2654 - accuracy: 0.9127 - val_loss: 1.8144 - val_accuracy: 0.5787\n",
      "Epoch 158/200\n",
      "449/449 [==============================] - 183s 408ms/step - loss: 0.2593 - accuracy: 0.9144 - val_loss: 2.0845 - val_accuracy: 0.5748\n",
      "Epoch 159/200\n",
      "449/449 [==============================] - 182s 406ms/step - loss: 0.2605 - accuracy: 0.9141 - val_loss: 2.0227 - val_accuracy: 0.5740\n",
      "Epoch 160/200\n",
      "449/449 [==============================] - 182s 406ms/step - loss: 0.2689 - accuracy: 0.9135 - val_loss: 1.9269 - val_accuracy: 0.5821\n",
      "Epoch 161/200\n",
      "449/449 [==============================] - 182s 406ms/step - loss: 0.2612 - accuracy: 0.9158 - val_loss: 1.9572 - val_accuracy: 0.5874\n",
      "Epoch 162/200\n",
      "449/449 [==============================] - 182s 406ms/step - loss: 0.2460 - accuracy: 0.9203 - val_loss: 1.8348 - val_accuracy: 0.5868\n",
      "Epoch 163/200\n",
      "449/449 [==============================] - 182s 406ms/step - loss: 0.2458 - accuracy: 0.9186 - val_loss: 2.0783 - val_accuracy: 0.5801\n",
      "Epoch 164/200\n",
      "449/449 [==============================] - 182s 406ms/step - loss: 0.2830 - accuracy: 0.9127 - val_loss: 2.0914 - val_accuracy: 0.5801\n",
      "Epoch 165/200\n",
      "449/449 [==============================] - 182s 405ms/step - loss: 0.2710 - accuracy: 0.9134 - val_loss: 2.0012 - val_accuracy: 0.5868\n",
      "Epoch 166/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "449/449 [==============================] - 182s 405ms/step - loss: 0.2644 - accuracy: 0.9177 - val_loss: 2.1031 - val_accuracy: 0.5768\n",
      "Epoch 167/200\n",
      "449/449 [==============================] - 182s 405ms/step - loss: 0.2593 - accuracy: 0.9209 - val_loss: 2.0555 - val_accuracy: 0.5726\n",
      "Epoch 168/200\n",
      "449/449 [==============================] - 182s 404ms/step - loss: 0.2560 - accuracy: 0.9195 - val_loss: 2.1986 - val_accuracy: 0.5784\n",
      "Epoch 169/200\n",
      "449/449 [==============================] - 182s 405ms/step - loss: 0.2557 - accuracy: 0.9206 - val_loss: 1.9873 - val_accuracy: 0.5756\n",
      "Epoch 170/200\n",
      "449/449 [==============================] - 182s 405ms/step - loss: 0.2629 - accuracy: 0.9171 - val_loss: 2.0624 - val_accuracy: 0.5762\n",
      "Epoch 171/200\n",
      "449/449 [==============================] - 183s 408ms/step - loss: 0.2475 - accuracy: 0.9187 - val_loss: 1.9313 - val_accuracy: 0.5751\n",
      "Epoch 172/200\n",
      "449/449 [==============================] - 182s 405ms/step - loss: 0.2705 - accuracy: 0.9122 - val_loss: 2.1178 - val_accuracy: 0.5784\n",
      "Epoch 173/200\n",
      "449/449 [==============================] - 182s 405ms/step - loss: 0.2500 - accuracy: 0.9208 - val_loss: 2.1090 - val_accuracy: 0.5731\n",
      "Epoch 174/200\n",
      "449/449 [==============================] - 182s 405ms/step - loss: 0.2561 - accuracy: 0.9172 - val_loss: 1.9186 - val_accuracy: 0.5812\n",
      "Epoch 175/200\n",
      "449/449 [==============================] - 183s 407ms/step - loss: 0.2343 - accuracy: 0.9248 - val_loss: 2.0569 - val_accuracy: 0.5754\n",
      "Epoch 176/200\n",
      "449/449 [==============================] - 168s 375ms/step - loss: 0.2431 - accuracy: 0.9222 - val_loss: 1.9555 - val_accuracy: 0.5706\n",
      "Epoch 177/200\n",
      "449/449 [==============================] - 168s 373ms/step - loss: 0.2528 - accuracy: 0.9173 - val_loss: 1.9796 - val_accuracy: 0.5862\n",
      "Epoch 178/200\n",
      "449/449 [==============================] - 167s 372ms/step - loss: 0.2485 - accuracy: 0.9201 - val_loss: 2.1808 - val_accuracy: 0.5823\n",
      "Epoch 179/200\n",
      "449/449 [==============================] - 167s 372ms/step - loss: 0.2647 - accuracy: 0.9154 - val_loss: 1.9573 - val_accuracy: 0.5818\n",
      "Epoch 180/200\n",
      "449/449 [==============================] - 167s 372ms/step - loss: 0.2556 - accuracy: 0.9161 - val_loss: 2.1314 - val_accuracy: 0.5720\n",
      "Epoch 181/200\n",
      "449/449 [==============================] - 167s 372ms/step - loss: 0.2456 - accuracy: 0.9225 - val_loss: 2.1385 - val_accuracy: 0.5720\n",
      "Epoch 182/200\n",
      "449/449 [==============================] - 167s 371ms/step - loss: 0.2757 - accuracy: 0.9109 - val_loss: 2.1148 - val_accuracy: 0.5807\n",
      "Epoch 183/200\n",
      "449/449 [==============================] - 167s 371ms/step - loss: 0.2524 - accuracy: 0.9201 - val_loss: 1.9966 - val_accuracy: 0.5754\n",
      "Epoch 184/200\n",
      "449/449 [==============================] - 166s 370ms/step - loss: 0.2588 - accuracy: 0.9206 - val_loss: 2.0641 - val_accuracy: 0.5751\n",
      "Epoch 185/200\n",
      "449/449 [==============================] - 167s 371ms/step - loss: 0.2494 - accuracy: 0.9212 - val_loss: 2.0364 - val_accuracy: 0.5795\n",
      "Epoch 186/200\n",
      "449/449 [==============================] - 167s 372ms/step - loss: 0.2506 - accuracy: 0.9219 - val_loss: 2.0542 - val_accuracy: 0.5648\n",
      "Epoch 187/200\n",
      "449/449 [==============================] - 167s 372ms/step - loss: 0.2563 - accuracy: 0.9190 - val_loss: 2.1579 - val_accuracy: 0.5704\n",
      "Epoch 188/200\n",
      "449/449 [==============================] - 167s 372ms/step - loss: 0.2256 - accuracy: 0.9277 - val_loss: 2.0928 - val_accuracy: 0.5670\n",
      "Epoch 189/200\n",
      "449/449 [==============================] - 167s 371ms/step - loss: 0.2438 - accuracy: 0.9211 - val_loss: 2.0692 - val_accuracy: 0.5673\n",
      "Epoch 190/200\n",
      "449/449 [==============================] - 166s 371ms/step - loss: 0.2431 - accuracy: 0.9215 - val_loss: 2.0461 - val_accuracy: 0.5759\n",
      "Epoch 191/200\n",
      "449/449 [==============================] - 167s 372ms/step - loss: 0.2566 - accuracy: 0.9200 - val_loss: 2.1041 - val_accuracy: 0.5770\n",
      "Epoch 192/200\n",
      "449/449 [==============================] - 171s 380ms/step - loss: 0.2474 - accuracy: 0.9199 - val_loss: 2.1780 - val_accuracy: 0.5756\n",
      "Epoch 193/200\n",
      "449/449 [==============================] - 170s 379ms/step - loss: 0.2470 - accuracy: 0.9221 - val_loss: 2.1727 - val_accuracy: 0.5779\n",
      "Epoch 194/200\n",
      "449/449 [==============================] - 174s 387ms/step - loss: 0.2558 - accuracy: 0.9225 - val_loss: 2.2140 - val_accuracy: 0.5818\n",
      "Epoch 195/200\n",
      "449/449 [==============================] - 167s 372ms/step - loss: 0.2407 - accuracy: 0.9269 - val_loss: 2.0797 - val_accuracy: 0.5715\n",
      "Epoch 196/200\n",
      "449/449 [==============================] - 167s 372ms/step - loss: 0.2541 - accuracy: 0.9201 - val_loss: 2.0442 - val_accuracy: 0.5712\n",
      "Epoch 197/200\n",
      "449/449 [==============================] - 168s 374ms/step - loss: 0.2421 - accuracy: 0.9201 - val_loss: 2.1714 - val_accuracy: 0.5745\n",
      "Epoch 198/200\n",
      "449/449 [==============================] - 167s 372ms/step - loss: 0.2360 - accuracy: 0.9250 - val_loss: 2.0655 - val_accuracy: 0.5784\n",
      "Epoch 199/200\n",
      "449/449 [==============================] - 168s 374ms/step - loss: 0.2484 - accuracy: 0.9253 - val_loss: 2.2807 - val_accuracy: 0.5779\n",
      "Epoch 200/200\n",
      "449/449 [==============================] - 167s 371ms/step - loss: 0.2261 - accuracy: 0.9260 - val_loss: 2.1843 - val_accuracy: 0.5726\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2b183f54c70>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, train_y,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, test_y),\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "arranged-ozone",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the  model to  use it later on\n",
    "fer_json2 = model.to_json()\n",
    "with open(\"fer2.json\", \"w\") as json_file:\n",
    "    json_file.write(fer_json2)\n",
    "model.save_weights(\"fer2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-suite",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
